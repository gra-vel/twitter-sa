---
title: "tw_part1"
author: "Gabriel Velastegui"
date: "3/2/2021"
output: 
  html_document:
    fig_height: 7
    fig_width: 9
    keep_md: yes
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

```
## Text Mining and twitter

### Summary
This project is my first attempt at text mining. Lately, I've been improving my knowledge of R and Python and wanted to start a project, using one of these two programming languages. I've decided to use R, since I feel more confident with some of its libraries. 

I use a dataset that contains tweets for four major news outlets. I've retrieved this data from Twitter's API using the rtweet library from R. The project consists of a simple EDA and text mining to identify...

For data wrangling, I use tools from dplyr, tidytext and lubridate. The analysis follows the first chapters of Text Mining with R from Julia Silge & David Robinson.

### Introduction
I'm usually reading newspapers from the US, mainly for the international news and economic analysis. It's evident that some news outlets are traditionally more focused on a specific topic, like politics or economics. The goal of this project is to visualize the similarities and differences between these news outlets.

With the ever-growing role of social media, I decided to use data from the timeline of some media outlets that I commonly use in Twitter. 

### Importing data

```{r}
library(rtweet)
library(tidyverse)
library(tidytext)
library(stopwords)
theme_set(theme_light())
color1 <- c("#EC7063", "#5DADE2", "#58D68D", "#AF7AC5")

library(scales) 
library(reactable) 
library(lubridate) 
library(corrplot)
library(wordcloud2)
```


```{r}
### Importing tweets
# api keys
# create_token(app = 'tw_recent',
#              consumer_key = "XXX",
#              consumer_secret = "XXX",
#              access_token = "XXX",
#              access_secret = "XXX")

# tw_retrieve <- get_timeline(c("WSJ","nytimes","business","FinancialTimes","washingtonpost"), n=3200)

# write_as_csv(tw_retrieve, "21_media.csv")

dim(tw_original)
names(tw_original)

```
Aqui necesito mostrar todas las variables y elegir solo las q voy a necesitar. Después mostrar cuantas observaciones hay. También vale la pena indicar lo que va a pasar con los retweets.

```{r}
### Overview
tw_original <- read_twitter_csv("C:/Users/G3/Documents/Gabriel/Profile/Projects/twitter-sa/21_media.csv")

tw_original %>%
  group_by(name) %>%
  summarize(n = n())
```
Considering the most popular media outlets in the US, I randomly picked five major newspapers to compare how different their timelines in Twitter are from one another. Financial Times is London-based, however it usually gives a large coverage of news related to the US. Moreover, the idea is to find how different is the content of the timelines from two groups of newspapers: the ones, which are known for their political reporting, from the ones that are known for their focus on business and economy. In this sense, I decided to classify these newspapers based on my own impression about the focus of each media outlet. Therefore, 'The New York Times' and 'The Washington Post' are considered as the politics-focused, whereas 'Financial Times' and 'The Wall Street Journal' are business-focused.

```{r}
# Filtering tweets and variables 
tw_media <- tw_original %>%
  mutate(date = as_datetime(created_at)) %>%
  filter(lang == "en") %>%
  select(name, date, text, source, is_retweet, favorite_count, 
         retweet_count, lang, geo_coords, status_id)

# Defining time period
tw_media %>%
  group_by(name) %>%
  mutate(n = n(),
         initial = min(date),
         diff = max(date) - min(date)) %>%
  select(name, n, initial, diff) %>% 
  unique()
```
Initially, I also wanted to use the timeline from Bloomberg, but decided to exclude it from the analysis after checking the timeline period of their tweets. It appears that the frequency of new tweets is higher in comparison to the rest of newspapers. This implies that the retrieved tweets from Bloomberg only cover a little less than two weeks, whereas the rest of newspapers go over a month. Therefore, I decided to remove those tweets. I also limited the time period to go from the first tweet of the New York Times timeline to the date these tweets were retrieved. It is important to consider that the defined period will not cover the entire retrieved timeline from The Washington Post by more than 10 days and The Wall Street Journal by more than five days.
The defined time period will cover tweets from 2020-12-31 until 2021-02-01.

```{r}
tw_media <- tw_media %>%
  filter(name != 'Bloomberg',
         date >= min(date[which(name == "The New York Times")]))

# Number of tweets for each outlet
tw_media %>%
  group_by(name) %>%
  summarise(n = n())

### Word frequency
# checking for retweets
tw_media %>%
  group_by(name, is_retweet) %>%
  count()
```
I considered deleting retweets, but due to the difference between the four outlets, I decided to keep them. Even though some tweets will repeat themselves, the point is too identify what kind of news where given more attention in the timeline and that includes retweets as well. Moreover, there are some outlets that tweet the same reports several times, but are not marked as retweets.

There are some things that should be considered in this part. I didn't exclude retweets as I plan to see the timeline as a whole. Usually, a tweet that starts with ' are deleted, because they are considered retweets. However, some tweets start with a quote and they are not necessarily a retweet.

```{r}
# data cleaning
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d])|'s|(?<=\\s)')" #to filter words

tw_words <- tw_media %>%
  #filter(is_retweet == FALSE) %>% 
  select(-geo_coords, -favorite_count, -retweet_count) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>% #deleting urls
  unnest_tokens(words, text, token = "regex", pattern = reg) %>%
  filter(!words %in% stop_words$word, 
         !str_detect(words, "^000"),
         str_detect(words, "[a-z]"))

# 10 most common words
tw_words %>%
  count(words) %>%
  arrange(desc(n)) %>%
  top_n(10, n) %>%
  ggplot(aes(reorder(words,n), n)) +
  geom_bar(stat = "identity", fill = "red", alpha = 0.6) + 
  ggtitle("Ten most common words") +
  xlab("Words") +
  ylab("n")
```
The three identifiable topics related to these words are the inauguration of the US president, the pandemic, the storming of the US capitol

```{r}
# number of total words
tw_words %>%
  group_by(name) %>%
  count(words) %>%
  mutate(total_words = sum(n),
         unique_words = n()) %>%
  select(name, total_words, unique_words) %>% unique() %>%
  ggplot(aes(x = name, y = total_words,  fill = name)) +
  geom_bar(stat = 'identity', show.legend = FALSE, alpha = 0.6) +
  geom_bar(aes(y = unique_words), stat = 'identity', show.legend = FALSE) +
  geom_text(aes(label = total_words), nudge_y = -1000) +
  geom_text(aes(y = unique_words, label = unique_words), nudge_y = -1000) +
  ggtitle('Number of total and unique words in tweets') +
  scale_fill_manual(values = color1) +
  xlab('') +
  ylab('Word count')
```
The NYT is the one with the most words followed by the WSJ. The darker color shows the number of unique words used in each timeline. At this level, there is more variability than in the count for total words???. However, it is important to consider that this variable is related to average length of the tweets for each outlet.

```{r}
# most used words by media
tw_words %>% 
  group_by(name) %>%
  count(words) %>%
  top_n(10) %>%
  ggplot(aes(reorder_within(words, n, name), n, fill = name)) +
  geom_col(show.legend = FALSE) +
  scale_x_reordered() +
  coord_flip() +
  facet_wrap(. ~ name, scales = "free") +
  ggtitle('Most frequent used word by media outlet') +
  scale_fill_manual(values = color1) +
  ylab('Word count') +
  xlab('Words')
```
Considering the most repeated words, the four outlets use almost the same words with different frequency. A notable difference might be the Financial Times, where 'uk' and 'china' appear only in this newspaper among the most frequent words. It is also noticeable that there is less variability in these words for the business-focused outlets, than for the politics-focused outlets. It seems that FT and the WSJ use 'covid' more often than 'coronavirus', in contrast to the NYT and the WP, when referring to the pandemic.

```{r, fig.height=7, fig.width=11}
### word frequencies as proportion
# Business-focused
frequency_eco <- tw_words %>%
  filter(!name %in% c("The New York Times", "The Washington Post")) %>%
  count(name, words) %>%
  group_by(name) %>%
  mutate(proportion = n/sum(n)) %>%
  select(-n) %>%
  spread(name, proportion)

frequency_eco %>%
  filter(`The Wall Street Journal` > 0.00025,
         `Financial Times` > 0.00025) %>%
  ggplot(aes(`The Wall Street Journal`, `Financial Times`, color = abs(`Financial Times` - `The Wall Street Journal`))) +
  geom_abline(color = "black") +
  geom_point(color = "#D35400", alpha = 0.25, size = 3, position = position_jitter(seed = 1), show.legend = FALSE) +
  geom_text(aes(label = words), check_overlap = TRUE, position = position_jitter(seed = 1), vjust = 1.5, show.legend = FALSE) + 
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(low = "#17202A", high = "#5D6D7E") +
  ggtitle("Word frequencies - Business")
```
This plot shows the correlation between word frequencies in FT and WSJ. Words such as 'trump', 'covid', 'biden' are the most frequent words and similarly used in both timelines. The word 'president' is more used in WSJ, whereas 'donald' appears more frequently in FT. Moreover, it is possible to identify clusters of words in both outlets that point to specific topics being covered more in one outlet than in other. For instance, the words 'gamestop', 'wall',  'street' and 'stock' appear more often in WSJ. Words like 'alexei', 'navalny', and 'europe' are more likely to appear in FT than in WSJ.

```{r, fig.height=7, fig.width=11}
# Politics-focused
frequency_pol <- tw_words %>%
  filter(name %in% c("The New York Times", "The Washington Post")) %>%
  count(name, words) %>%
  group_by(name) %>%
  mutate(proportion = n/sum(n)) %>%
  select(-n) %>%
  spread(name, proportion)

frequency_pol %>%
  filter(`The Washington Post` > 0.00025,
         `The New York Times` > 0.00025) %>%
  ggplot(aes(`The Washington Post`, `The New York Times`, color = abs(`The New York Times` - `The Washington Post`))) +
  geom_abline(color = "black") +
  geom_point(color = "#D35400", alpha = 0.25, size = 3, position = position_jitter(seed = 1), show.legend = FALSE) +
  geom_text(aes(label = words), check_overlap = TRUE, position = position_jitter(seed = 1), vjust = 1.5, show.legend = FALSE) + 
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(low = "#17202A", high = "#5D6D7E") +
  ggtitle("Word frequencies - Politics")
```
In the case of politics-focused media outlets, the correlation is visually less spread out than in the previous case. There are few noticeable clusters of outliers, since most words are close to the reference line. The word that stands out is 'trump' with a higher proportion appearing in the WP, than in the NYT.

```{r, fig.height=7, fig.width=11}
# Total
frequency_total <- tw_words %>%
  count(name, words) %>%
  group_by(name) %>%
  mutate(proportion = n/sum(n)) %>%
  select(-n) %>%
  spread(name, proportion) %>%
  gather(name_eco, proportion_eco, c('Financial Times','The Wall Street Journal')) %>%
  gather(name_pol, proportion_pol, c('The New York Times', 'The Washington Post'))

frequency_total %>%
  filter(proportion_eco > 0.00025,
         proportion_pol > 0.00025) %>%
  ggplot(aes(proportion_eco, proportion_pol, color = abs(proportion_eco - proportion_pol))) +
  geom_abline(color = "black") +
  geom_point(color = "#D35400", alpha = 0.25, size = 3, position = position_jitter(seed = 1), show.legend = FALSE) +
  geom_text(data = subset(frequency_total, proportion_eco >= 0.0005 & proportion_pol >= 0.0005), 
            aes(label = words), check_overlap = TRUE, position = position_jitter(seed = 1), vjust = 1.5, show.legend = FALSE) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(low = "#17202A", high = "#5D6D7E") +
  ggtitle("Word frequencies - Business & Politics") +
  xlab("Business-focused") +
  ylab("Politics-focused") +
  facet_grid(name_eco ~ name_pol)
```
When comparing between the two groups of media outlets the word 'trump' and words related to the pandemic are the most common in terms of frequency. Visually, it appears that the frequency of words is more spread out when comparing the WP to the two business-focused outlets, especially with FT. This would point out the different coverage in topics. However, it appears that there is a close correlation between the NYT and the WSJ. 

```{r}
# word correlation
corr_media <- tw_words %>%
  count(name, words) %>%
  group_by(name) %>%
  mutate(proportion = n/sum(n)) %>%
  select(-n) %>%
  spread(name, proportion) %>%
  drop_na() %>%
  select(-words) %>%
  cor()

corrplot(corr_media, method = "number", type = "upper", 
         col = colorRampPalette(c("black", "black", "black", "white", "darkgreen"))(20), cl.lim = c(0.5, 1)) 
```
A correlation matrix of word frequencies shows that the NYT and WSJ are in fact the most similar media outlets from this sample of tweets. The politics-focused outlets have a coefficient of 0.75, whereas the business-focused have a coefficient of 0.77. The rest of cases are under 0.70.

```{r}
# frequency and timeline
tw_media %>%
  ggplot(aes(as.Date(date), fill = name)) +  
  geom_histogram(position = 'identity', binwidth = 1, show.legend = FALSE, alpha = 0.65) +
  geom_histogram(data = subset(tw_media %>%
                                 mutate(wknds = wday(date)), 
                               wknds==7 | wknds==1), # Saturday and Sunday
                 binwidth = 1, alpha = 0.99, show.legend = FALSE) +
  ggtitle("Frequency of tweets by day") +
  xlab("Date") +
  ylab("Count") +
  scale_fill_manual(values = color1) +
  facet_wrap(.~ name, ncol = 1)
```
As for the tweets frequency, it looks like the outlets all share an almost uniform distribution with some exceptions. The darker areas represent the weekends with fewer tweets in comparison to weekdays. When removing retweets, the distribution remains the same, which suggests that retweets are more or less evenly distributed along the timeline.

```{r}
# comparison
tw_media %>%
  mutate(date = as_date(date)) %>%
  group_by(name, date) %>%
  mutate(n = n()) %>%
  ggplot(aes(date, n, color = name)) +
  geom_line(size = 1) +
  ggtitle("Frequency of tweets by day") +
  xlab("Date") +
  ylab("Count") +
  scale_color_manual(values = color1) +
  theme(legend.position = "bottom",
        legend.title = element_blank())
```
Visually, it looks like the two politics-focused media outlets have a higher average rate of tweets on weekdays than the business-focused outlets.

```{r}
# daytime
tw_media %>%
  count(name, hour = hour(with_tz(date, 'EST'))) %>% 
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(hour, n, fill = name)) +
  geom_bar(stat = 'identity', show.legend = FALSE, alpha = 0.5) +
  geom_bar(data = subset(tw_media %>%
                           filter(is_retweet == FALSE) %>%
                           count(name, hour = lubridate::hour(with_tz(date, 'EST'))) %>%
                           mutate(percent = n / sum(n))),
           stat = 'identity', show.legend = FALSE) +
  scale_x_continuous(breaks = unique(hour(with_tz(tw_media$date, 'EST')))) +
  ggtitle("Time of the day for tweets (EST)") +
  xlab("Hour") +
  ylab("Count") +
  scale_fill_manual(values = color1) +
  facet_wrap(. ~ name, ncol = 1)
```
For The New Yortk Times and The Washington Post is easier to distinguish the time of the day, where more tweets are published, than for the other two outlets. It is important to consider that the time for each tweet is adjusted for EST. Besides, the Financial Times is a London-based newspaper, which is why it follows a different pattern. Moreover, it is also worth noting that the darker color represents the timeline without retweets, whereas the light color are the added retweets. This shows that the time of the day with more tweets is also the period with more retweets.

```{r}
### lenght and number of words
# length of tweet
tw_media %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;|(?<=)<(.*?)(?=>)>", "")) %>% #removes url and emojis
  mutate(length = nchar(text)) %>%
  ggplot(aes(x=name, y=length, fill = name)) +
  geom_boxplot(show.legend = FALSE) +
  ggtitle("Tweet length") +
  xlab("") +
  ylab("Length")
```
In terms of lenght, the NYT has a higher median than the rest of outlets. The WP has the lowest median and a group of outliers at both ends of the distribution. As most of those outliers are on the higher end of the distribution I checked a random sample of tweets with more than 150 characters to see if there is any reason as to why they are longer than average. It doesn't appear to be a clear trend other than these tweets sometimes are quoting an individual.

```{r}
# reactable
tw_media %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;|(?<=)<(.*?)(?=>)>", "")) %>%
  mutate(length = nchar(text)) %>%
  filter(length > 150 & name == "The Washington Post") %>%
  select(date, text, length) %>%
  sample_n(20, seed = 1) %>%
  reactable()
```

```{r}
#distribution according status_id
tw_media %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;|(?<=)<(.*?)(?=>)>", "")) %>%
  mutate(length = nchar(text),
         date_simp = date(with_tz(date, 'EST')),
         hour = hour(with_tz(date, 'EST'))) %>%
  group_by(name, date_simp, hour) %>%
  mutate(length_hour = mean(length),
         date = paste(date_simp,hour, sep = "-")) %>%
  select(name, date, date_simp, hour, length_hour) %>% unique() %>%
  ggplot(aes(x=date, y=length_hour, fill = length_hour)) +
  geom_bar(stat = 'identity') +
  ggtitle("Average length of tweets per hour") +
  xlab("Hours") +
  ylab("Average length") +
  scale_fill_gradient(low = "yellow", high = "blue") +
  facet_wrap(.~name, ncol = 1) +
  theme(panel.background = element_rect(fill = "white"),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank())
```

```{r}
### wordcloud
tw_words %>%
  rename(word = words) %>%
  filter(!word %in% c('opinion','ft','edition','times','story','front','page',
                      'read','writes','breaking','analysis','week',
                      'york','#wsjwhatsnow', '@wsjopinion')) %>%
  count(word, sort = TRUE) %>%
  top_n(1000) %>%
  wordcloud2(size = 1, shape = 'square')
```



















